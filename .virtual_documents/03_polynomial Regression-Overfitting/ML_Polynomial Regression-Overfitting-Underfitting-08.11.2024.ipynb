





# bu notebook overfitting-underfitting konularını daha iyi anlamak üzere oluşturulmuştur





















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams["figure.figsize"] = (10,6)





from sklearn.preprocessing import PolynomialFeatures


#örnek data seti üzeirnden basitçe degerlendirelim

data = np.array([[2,3,4]])
print(data)


trans = PolynomialFeatures(degree=2, include_bias=False)  # data'ya 2.dereceden Polynomial features ekleyeceğim


trans.fit(data)


trans.transform(data)     # 2, 3, 4 , 2x3, 2x4, 3x4, 2**2, 3**2, 4**2 bu şekilde 2.dereceye çevirdi transform işlemi tamamladı.

# X1 ** 2 + X2 ** 2 + X3 ** 2 + X1X2 + X1X3 + X2X3 + X1 + X2 + X3 


# hem transformer hem de fit işlemini aynı anda yapmak için alttaki kodu kullanıypruz

trans.fit_transform(data)


# degree = 3 olsaydı reger;
# 2, 3, 4, 2x3, 2x4, 3x4, 2**2, 3**2, 4**2, 2x3x4, 3x2**2, 4x2**2, 2x3**2, 4x3**2, 2x4**2, 3x4**2, 2**3, 3**3, 4**3





df = pd.read_csv("Advertising.csv")
df


# şimdi data setimizin feature sayısını matematiksel birşekilde artırmak için Polynomial Features fonklsiyonu kullanalım







X = df.drop("sales", axis=1)
y = df.sales


poly_conv = PolynomialFeatures(degree=2, include_bias=False)  


poly_conv.fit(X)


poly_features = poly_conv.transform(X)   # polymomial feature lar uretttik


poly_features


poly_features.shape    # satır saysıı 200 kaldı, feature 3'ten 9' a çıkarıldı


pd.DataFrame(poly_features, columns=["TV", "radio", "newspaper", "TV^2", "TV&radio", \
                                    "TV&newspaper", "radio^2", "radio&newspaper", "newspaper^2"]).head()


X.shape   # ilk halini hatırlayalım





from sklearn.model_selection  import train_test_split


X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)

# X yerine poly_features ı alarak 9 feature olan data setini tercih ettik





from sklearn.linear_model import LinearRegression


model_poly = LinearRegression()


model_poly.fit(X_train, y_train)


y_pred = model_poly.predict(X_test)


my_dict = {"Actual":y_test, "pred":y_pred, "residual": y_test - y_pred }
compare = pd.DataFrame(my_dict)
compare.head(20)


compare.head(20).plot(kind ="bar", figsize =(15,9))
plt.show();





model_poly.coef_


# coef 'ları Df haline getirelim önceki feature larla esleştierlim'

df_coef = pd.DataFrame(model_poly.coef_, index = ["TV", "radio", "newspaper", "TV^2", "TV&radio", \
                                    "TV&newspaper", "radio^2", "radio&newspaper", "newspaper^2"], columns = ["coef"])


df_coef


# ilk satırı manuel olarak prediction satırına koyalım..alttaki resimde de ilk satır degerleri görülüyor

model_poly.predict([[2.301000e+02, 3.780000e+01, 6.920000e+01, 5.294601e+04,
       8.697780e+03, 1.592292e+04, 1.428840e+03, 2.615760e+03,
       4.788640e+03]]) 








from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score


y_pred = model_poly.predict(X_test)   


def eval_metric(actual, pred):
    mae = mean_absolute_error(actual, pred)
    mse = mean_squared_error(actual, pred)
    rmse = np.sqrt(mean_squared_error(actual, pred))
    R2_score = r2_score(actual, pred)
    print("Model testing performance:")
    print("--------------------------")
    print(f"R2_score \t: {R2_score}")
    print(f"MAE \t\t: {mae}")
    print(f"MSE \t\t: {mse}")
    print(f"RMSE \t\t: {rmse}")


eval_metric(y_test, y_pred)  # Test kısmının degerleri


y_train_pred = model_poly.predict(X_train)


eval_metric(y_train, y_train_pred)








# RMSE degerine göre optimum dereceye karar verecegiz


def poly(d):   # degree derecesi içine ne yazarsam ona göre çözüm oluşturacak
    
    train_rmse_errors = []   # trains rmse değerlerin toplayacak
    test_rmse_errors = []    # test rmse değerlerin toplayacak
    number_of_features = []  # derece arttıkça feature sayısının kaça çıktığnı göreceğiz
    
    for i in range(1, d):  # 1 den kaçıncı dereceye kadar istiyorsak ona göre yazacağız
        polynomial_converter = PolynomialFeatures(degree = i, include_bias =False)
        poly_features = polynomial_converter.fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)
        
        model = LinearRegression(fit_intercept=True) 
        model.fit(X_train, y_train)
        
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)
        
        train_RMSE = np.sqrt(mean_squared_error(y_train,train_pred))
        test_RMSE = np.sqrt(mean_squared_error(y_test,test_pred))
        
        train_rmse_errors.append(train_RMSE)
        test_rmse_errors.append(test_RMSE)
        
        number_of_features.append(poly_features.shape[1])
        
    return pd.DataFrame({"train_rmse_errors": train_rmse_errors, "test_rmse_errors":test_rmse_errors, "number of features":number_of_features}, 
                        index=range(1,d))


poly(10)


# yukardakş sonucların grafigini çizelim ve patlama noktasını görelim

plt.plot(range(1,10), poly(10)["train_rmse_errors"], label = "TRAIN")
plt.plot(range(1,10), poly(10)["test_rmse_errors"], label = "TEST")
plt.xlabel("Polynamial Complex")
plt.ylabel("RMSE")
plt.legend();
plt.show()


# grafige daha yaakından bakalım
plt.plot(range(1,6), poly(6)["train_rmse_errors"], label = "TRAIN")
plt.plot(range(1,6), poly(6)["test_rmse_errors"], label = "TEST")
plt.xlabel("Polynamial Complex")
plt.ylabel("RMSE")
plt.legend();
plt.xticks(range(1, 6, 1))
plt.show()


# patlama noktasının bir solundaki değerde kalmak daha güvenilir olacaktır.
# bu nedenle 3.derece optimum deger olarak alınaiblir














# optimum derecenin 3.derece oldugunu kabul ederek devamedleim, final modeli bu bilgiyle olusturalım
# modeli artık butun data ile egiterek daha iyi sonuclar elde etmek istiyoruz. Train-test kısmı artık yok

final_poly_conv = PolynomialFeatures(degree=3, include_bias=False)

# 3.derece ile modelimizi egitecegiz..train edecegiz...


final_model = LinearRegression()


final_model.fit(final_poly_conv.fit_transform(X), y)

# modelimi fit ediyorum, X yerine Polymomial Feature kullanıp 3. dereec için feature uretiyorum.
# aldıgım final_poly_conv degiskeni verip datanın hepsini (X) ile fit ediyorum





new_record = [[150,20,15]]   # prediction yaparken başlangıçtaki feature sayusı kadar veriyoruz


new_record_poly = final_poly_conv.fit_transform(new_record)   # polynomial feature a çevirdik


new_record_poly


final_model.predict(new_record_poly)  # yeni durumu final_model bilgileri ile predict ettik








# Overfitting in yuklardaki grafikte başladığı nokta olan (train ve test in ayrıştığı yer) 5. dereceyi esas alarak feature  üretelim
# bilebile overfitting oluşumunu izleyelim


over_poly_converter = PolynomialFeatures(degree=5, include_bias=False)

# 5.dereceden polynomial feature uretiyoruz


over_model =LinearRegression()   


X_train, X_test, y_train, y_test = train_test_split(over_poly_converter.fit_transform(X), y, test_size=0.3, random_state=101)


over_model.fit(X_train, y_train)


y_pred_over = over_model.predict(X_test)


eval_metric(y_test, y_pred_over)


# train skorlarına bakalım
y_train_over = over_model.predict(X_train)


eval_metric(y_train, y_train_over)


# train R2 si olan 0.99 ile test R2 si olan 0.76 arasında ciddi bir fark var. Bu fark bize overfitting i gösterdi
# train setin skorları çok iyi olmasına ragmen test setinin skorları kötu oldugu için overfitting teşhisi konuldu












